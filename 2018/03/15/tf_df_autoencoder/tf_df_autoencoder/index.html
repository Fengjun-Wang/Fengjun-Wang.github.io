<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="learning," />










<meta name="description" content="123import pandas as pdimport numpy as npimport tensorflow as tf 1mal_file = &apos;~/Documents/tensorflow_training/hkt/data/malicious_win13_binetflow_339.csv&apos; 1mal_339 = pd.read_csv(mal_file) 12mal_339.head">
<meta name="keywords" content="learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Hackathon - Anomaly detection (botnet)">
<meta property="og:url" content="http://yoursite.com/2018/03/15/tf_df_autoencoder/tf_df_autoencoder/index.html">
<meta property="og:site_name" content="JunJun&#39;s farm">
<meta property="og:description" content="123import pandas as pdimport numpy as npimport tensorflow as tf 1mal_file = &apos;~/Documents/tensorflow_training/hkt/data/malicious_win13_binetflow_339.csv&apos; 1mal_339 = pd.read_csv(mal_file) 12mal_339.head">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2018/03/15/tf_df_autoencoder/tf_df_autoencoder/output_23_0.png">
<meta property="og:image" content="http://yoursite.com/2018/03/15/tf_df_autoencoder/tf_df_autoencoder/output_24_0.png">
<meta property="og:updated_time" content="2018-03-15T22:37:31.751Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hackathon - Anomaly detection (botnet)">
<meta name="twitter:description" content="123import pandas as pdimport numpy as npimport tensorflow as tf 1mal_file = &apos;~/Documents/tensorflow_training/hkt/data/malicious_win13_binetflow_339.csv&apos; 1mal_339 = pd.read_csv(mal_file) 12mal_339.head">
<meta name="twitter:image" content="http://yoursite.com/2018/03/15/tf_df_autoencoder/tf_df_autoencoder/output_23_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/03/15/tf_df_autoencoder/tf_df_autoencoder/"/>





  <title>Hackathon - Anomaly detection (botnet) | JunJun's farm</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">JunJun's farm</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/15/tf_df_autoencoder/tf_df_autoencoder/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jun">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JunJun's farm">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Hackathon - Anomaly detection (botnet)</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-15T23:37:31+01:00">
                2018-03-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/03/15/tf_df_autoencoder/tf_df_autoencoder/" class="leancloud_visitors" data-flag-title="Hackathon - Anomaly detection (botnet)">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mal_file = <span class="string">'~/Documents/tensorflow_training/hkt/data/malicious_win13_binetflow_339.csv'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mal_339 = pd.read_csv(mal_file)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mal_339.head(<span class="number">5</span>)</span><br><span class="line">type(mal_339)</span><br></pre></td></tr></table></figure>
<pre><code>pandas.core.frame.DataFrame
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mal_339.dtypes</span><br></pre></td></tr></table></figure>
<pre><code>Unnamed: 0      int64
StartTime      object
Dur           float64
Proto          object
SrcAddr        object
Sport         float64
Dir            object
DstAddr        object
Dport         float64
State          object
sTos          float64
dTos          float64
TotPkts         int64
TotBytes        int64
SrcBytes        int64
Label         float64
dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Dir: categorical</span></span><br><span class="line">one_hot = pd.get_dummies(mal_339[<span class="string">'Dir'</span>])</span><br><span class="line"><span class="comment"># mal_339['Dir']=mal_339['Dir'].apply()</span></span><br><span class="line">mal_339 = mal_339.join(one_hot)</span><br><span class="line">mal_339.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>Unnamed: 0</th><br>      <th>StartTime</th><br>      <th>Dur</th><br>      <th>Proto</th><br>      <th>SrcAddr</th><br>      <th>Sport</th><br>      <th>Dir</th><br>      <th>DstAddr</th><br>      <th>Dport</th><br>      <th>State</th><br>      <th>…</th><br>      <th>dTos</th><br>      <th>TotPkts</th><br>      <th>TotBytes</th><br>      <th>SrcBytes</th><br>      <th>Label</th><br>      <th>-&gt;</th><br>      <th>&lt;-</th><br>      <th>&lt;-&gt;</th><br>      <th>&lt;?&gt;</th><br>      <th>who</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>1970/01/01 01:00:00.000000</td><br>      <td>0.000000</td><br>      <td>llc</td><br>      <td>00:00:00:00:00:00</td><br>      <td>0.0</td><br>      <td>-&gt;</td><br>      <td>00:00:00:00:00:00</td><br>      <td>0.0</td><br>      <td>INT</td><br>      <td>…</td><br>      <td>NaN</td><br>      <td>1</td><br>      <td>60</td><br>      <td>60</td><br>      <td>NaN</td><br>      <td>1</td><br>      <td>0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>1970/01/01 01:00:10.929202</td><br>      <td>2.002936</td><br>      <td>arp</td><br>      <td>0.0.0.0</td><br>      <td>NaN</td><br>      <td>who</td><br>      <td>192.168.1.119</td><br>      <td>NaN</td><br>      <td>INT</td><br>      <td>…</td><br>      <td>NaN</td><br>      <td>3</td><br>      <td>126</td><br>      <td>126</td><br>      <td>NaN</td><br>      <td>0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>1</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>1970/01/01 01:00:10.929400</td><br>      <td>0.000104</td><br>      <td>ipv6-icmp</td><br>      <td>::</td><br>      <td>135.0</td><br>      <td>-&gt;</td><br>      <td>ff02::1:ff21:ab75</td><br>      <td>0.0</td><br>      <td>NNS</td><br>      <td>…</td><br>      <td>NaN</td><br>      <td>2</td><br>      <td>156</td><br>      <td>156</td><br>      <td>NaN</td><br>      <td>1</td><br>      <td>0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>4</td><br>      <td>1970/01/01 01:00:10.929570</td><br>      <td>0.000000</td><br>      <td>ipv6-icmp</td><br>      <td>::</td><br>      <td>135.0</td><br>      <td>-&gt;</td><br>      <td>ff02::1:ff90:cb4a</td><br>      <td>0.0</td><br>      <td>NNS</td><br>      <td>…</td><br>      <td>NaN</td><br>      <td>1</td><br>      <td>78</td><br>      <td>78</td><br>      <td>NaN</td><br>      <td>1</td><br>      <td>0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>5</td><br>      <td>1970/01/01 01:00:10.929638</td><br>      <td>1645.342285</td><br>      <td>ipv6-icmp</td><br>      <td>::</td><br>      <td>135.0</td><br>      <td>-&gt;</td><br>      <td>ff02::1:ff00:994</td><br>      <td>0.0</td><br>      <td>NNS</td><br>      <td>…</td><br>      <td>NaN</td><br>      <td>3</td><br>      <td>234</td><br>      <td>234</td><br>      <td>NaN</td><br>      <td>1</td><br>      <td>0</td><br>      <td>0</td><br>      <td>0</td><br>      <td>0</td><br>    </tr><br>  </tbody><br></table><br><p>5 rows × 21 columns</p><br></div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#StartTime: features</span></span><br><span class="line">mal_339[<span class="string">'StartTime'</span>] = pd.to_datetime(mal_339[<span class="string">'StartTime'</span>], format=<span class="string">'%Y/%m/%d %H:%M:%S.%f'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mal_339.dtypes</span></span><br><span class="line">one_hot_proto = pd.get_dummies(mal_339[<span class="string">'Proto'</span>])</span><br><span class="line">mal_339 = mal_339.join(one_hot_proto)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mal_339[<span class="string">'Label'</span>]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dur           float64</span></span><br><span class="line"><span class="comment"># Proto          object</span></span><br><span class="line"><span class="comment"># SrcAddr        object</span></span><br><span class="line"><span class="comment"># Sport         float64</span></span><br><span class="line"><span class="comment"># Dir            object</span></span><br><span class="line"><span class="comment"># DstAddr        object</span></span><br><span class="line"><span class="comment"># Dport         float64</span></span><br><span class="line"><span class="comment"># State          object</span></span><br><span class="line"><span class="comment"># sTos          float64</span></span><br><span class="line"><span class="comment"># dTos          float64</span></span><br><span class="line"><span class="comment"># TotPkts         int64</span></span><br><span class="line"><span class="comment"># TotBytes        int64</span></span><br><span class="line"><span class="comment"># SrcBytes        int64</span></span><br><span class="line"><span class="comment"># Label</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">one_hot_state = pd.get_dummies(mal_339[<span class="string">'State'</span>])</span><br><span class="line">mal_339 = mal_339.join(one_hot_state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FEATURES</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FEATURES=mal_339.columns.tolist()</span><br><span class="line">FEATURES</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;Unnamed: 0&apos;,
 &apos;StartTime&apos;,
 &apos;Dur&apos;,
 &apos;Proto&apos;,
 &apos;SrcAddr&apos;,
 &apos;Sport&apos;,
 &apos;Dir&apos;,
 &apos;DstAddr&apos;,
 &apos;Dport&apos;,
 &apos;State&apos;,
 &apos;sTos&apos;,
 &apos;dTos&apos;,
 &apos;TotPkts&apos;,
 &apos;TotBytes&apos;,
 &apos;SrcBytes&apos;,
 &apos;Label&apos;,
 &apos;   -&gt;&apos;,
 &apos;  &lt;-&apos;,
 &apos;  &lt;-&gt;&apos;,
 &apos;  &lt;?&gt;&apos;,
 &apos;  who&apos;,
 &apos;2114&apos;,
 &apos;35033&apos;,
 &apos;45647&apos;,
 &apos;arp&apos;,
 &apos;ib&apos;,
 &apos;icmp&apos;,
 &apos;igmp&apos;,
 &apos;ipv6-icmp&apos;,
 &apos;llc&apos;,
 &apos;pim&apos;,
 &apos;pipe&apos;,
 &apos;pppoe&apos;,
 &apos;rtcp&apos;,
 &apos;tcp&apos;,
 &apos;udp&apos;,
 &apos;CON&apos;,
 &apos;ECO&apos;,
 &apos;FSA_FSA&apos;,
 &apos;FSA_FSPA&apos;,
 &apos;FSPA_FSPA&apos;,
 &apos;FSRPA_FSPA&apos;,
 &apos;FSRPA_SA&apos;,
 &apos;FSRPA_SPA&apos;,
 &apos;INT&apos;,
 &apos;MHR&apos;,
 &apos;MRQ&apos;,
 &apos;NNS&apos;,
 &apos;NRS&apos;,
 &apos;PA_R&apos;,
 &apos;PPO&apos;,
 &apos;RED&apos;,
 &apos;REQ&apos;,
 &apos;RSP&apos;,
 &apos;SPA_SPA&apos;,
 &apos;SRA_SA&apos;,
 &apos;SRPA_FSPA&apos;,
 &apos;SRPA_SA&apos;,
 &apos;SRPA_SPA&apos;,
 &apos;SR_SA&apos;,
 &apos;S_&apos;,
 &apos;S_RA&apos;,
 &apos;TXD&apos;,
 &apos;UNK&apos;,
 &apos;URFIL&apos;,
 &apos;URH&apos;,
 &apos;URHPRO&apos;,
 &apos;URN&apos;,
 &apos;URP&apos;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">should_remove = [<span class="string">'Unnamed: 0'</span>,<span class="string">'StartTime'</span>, <span class="string">'Proto'</span>, <span class="string">'Dir'</span>, <span class="string">'State'</span>, <span class="string">'SrcAddr'</span>, <span class="string">'Dport'</span>, <span class="string">'DstAddr'</span>, <span class="string">'sTos'</span>, <span class="string">'dTos'</span>, <span class="string">'Label'</span>]</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> should_remove:</span><br><span class="line">    <span class="keyword">print</span> item</span><br><span class="line">    FEATURES.remove(item)</span><br><span class="line">FEATURES</span><br></pre></td></tr></table></figure>
<pre><code>Unnamed: 0
StartTime
Proto
Dir
State
SrcAddr
Dport
DstAddr
sTos
dTos
Label





[&apos;Dur&apos;,
 &apos;Sport&apos;,
 &apos;TotPkts&apos;,
 &apos;TotBytes&apos;,
 &apos;SrcBytes&apos;,
 &apos;   -&gt;&apos;,
 &apos;  &lt;-&apos;,
 &apos;  &lt;-&gt;&apos;,
 &apos;  &lt;?&gt;&apos;,
 &apos;  who&apos;,
 &apos;2114&apos;,
 &apos;35033&apos;,
 &apos;45647&apos;,
 &apos;arp&apos;,
 &apos;ib&apos;,
 &apos;icmp&apos;,
 &apos;igmp&apos;,
 &apos;ipv6-icmp&apos;,
 &apos;llc&apos;,
 &apos;pim&apos;,
 &apos;pipe&apos;,
 &apos;pppoe&apos;,
 &apos;rtcp&apos;,
 &apos;tcp&apos;,
 &apos;udp&apos;,
 &apos;CON&apos;,
 &apos;ECO&apos;,
 &apos;FSA_FSA&apos;,
 &apos;FSA_FSPA&apos;,
 &apos;FSPA_FSPA&apos;,
 &apos;FSRPA_FSPA&apos;,
 &apos;FSRPA_SA&apos;,
 &apos;FSRPA_SPA&apos;,
 &apos;INT&apos;,
 &apos;MHR&apos;,
 &apos;MRQ&apos;,
 &apos;NNS&apos;,
 &apos;NRS&apos;,
 &apos;PA_R&apos;,
 &apos;PPO&apos;,
 &apos;RED&apos;,
 &apos;REQ&apos;,
 &apos;RSP&apos;,
 &apos;SPA_SPA&apos;,
 &apos;SRA_SA&apos;,
 &apos;SRPA_FSPA&apos;,
 &apos;SRPA_SA&apos;,
 &apos;SRPA_SPA&apos;,
 &apos;SR_SA&apos;,
 &apos;S_&apos;,
 &apos;S_RA&apos;,
 &apos;TXD&apos;,
 &apos;UNK&apos;,
 &apos;URFIL&apos;,
 &apos;URH&apos;,
 &apos;URHPRO&apos;,
 &apos;URN&apos;,
 &apos;URP&apos;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_input_fn</span><span class="params">(df)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pandas_to_tf</span><span class="params">(pdcol)</span>:</span></span><br><span class="line">    <span class="comment"># convert the pandas column values to float</span></span><br><span class="line">    t = tf.constant(pdcol.astype(<span class="string">'float32'</span>).values)</span><br><span class="line">    <span class="comment"># take the column which is of shape (N) and make it (N, 1)</span></span><br><span class="line">    <span class="keyword">return</span> tf.expand_dims(t, <span class="number">-1</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># create features, columns</span></span><br><span class="line">    features = &#123;k: pandas_to_tf(df[k]) <span class="keyword">for</span> k <span class="keyword">in</span> FEATURES&#125;</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line">  <span class="keyword">return</span> input_fn</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_feature_cols</span><span class="params">()</span>:</span></span><br><span class="line">  input_columns = [tf.contrib.layers.real_valued_column(k) <span class="keyword">for</span> k <span class="keyword">in</span> FEATURES]</span><br><span class="line">  <span class="keyword">return</span> input_columns</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line">display_step = <span class="number">10</span></span><br><span class="line">examples_to_show = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">num_hidden_1 = <span class="number">100</span> <span class="comment"># 1st layer num features</span></span><br><span class="line">num_hidden_2 = <span class="number">30</span> <span class="comment"># 2nd layer num features (the latent dim)</span></span><br><span class="line">num_input = len(FEATURES) <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line"></span><br><span class="line">mal_339.shape</span><br><span class="line"><span class="keyword">print</span> len(FEATURES)</span><br><span class="line">batch_num = mal_339.shape[<span class="number">0</span>]/batch_size</span><br><span class="line">num_steps = mal_339.shape[<span class="number">0</span>]/batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph input (only pictures)</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'input'</span>):</span><br><span class="line">    X = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, num_input])</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">    weights = &#123;</span><br><span class="line">    <span class="string">'encoder_h1'</span>: tf.Variable(tf.random_normal([num_input, num_hidden_1])),</span><br><span class="line">    <span class="string">'encoder_h2'</span>: tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),</span><br><span class="line">    <span class="string">'decoder_h1'</span>: tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),</span><br><span class="line">    <span class="string">'decoder_h2'</span>: tf.Variable(tf.random_normal([num_hidden_1, num_input])),</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">    biases = &#123;</span><br><span class="line">    <span class="string">'encoder_b1'</span>: tf.Variable(tf.random_normal([num_hidden_1])),</span><br><span class="line">    <span class="string">'encoder_b2'</span>: tf.Variable(tf.random_normal([num_hidden_2])),</span><br><span class="line">    <span class="string">'decoder_b1'</span>: tf.Variable(tf.random_normal([num_hidden_1])),</span><br><span class="line">    <span class="string">'decoder_b2'</span>: tf.Variable(tf.random_normal([num_input])),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<pre><code>58
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Building the encoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># Encoder Hidden layer with sigmoid activation #1</span></span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[<span class="string">'encoder_h1'</span>]),</span><br><span class="line">                                   biases[<span class="string">'encoder_b1'</span>]))</span><br><span class="line">    <span class="comment"># Encoder Hidden layer with sigmoid activation #2</span></span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[<span class="string">'encoder_h2'</span>]),</span><br><span class="line">                                   biases[<span class="string">'encoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Building the decoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># Decoder Hidden layer with sigmoid activation #1</span></span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[<span class="string">'decoder_h1'</span>]),</span><br><span class="line">                                   biases[<span class="string">'decoder_b1'</span>]))</span><br><span class="line">    <span class="comment"># Decoder Hidden layer with sigmoid activation #2</span></span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[<span class="string">'decoder_h2'</span>]),</span><br><span class="line">                                   biases[<span class="string">'decoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Construct model</span></span><br><span class="line">encoder_op = encoder(X)</span><br><span class="line">decoder_op = decoder(encoder_op)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prediction</span></span><br><span class="line">y_pred = decoder_op</span><br><span class="line"><span class="comment"># Targets (Labels) are the input data.</span></span><br><span class="line">y_true = X</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss and optimizer, minimize the squared error</span></span><br><span class="line">loss = tf.reduce_mean(tf.pow(y_true - y_pred, <span class="number">2</span>))</span><br><span class="line">optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">log_dir = <span class="string">'~/Documents/tensorflow_training/hkt'</span></span><br><span class="line"></span><br><span class="line">train_writer = tf.summary.FileWriter(log_dir + <span class="string">'/train'</span>, sess.graph)</span><br><span class="line">test_writer = tf.summary.FileWriter(log_dir + <span class="string">'/test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">sess.run(init)</span><br><span class="line">sess.run(weights[<span class="string">'encoder_h1'</span>][:<span class="number">20</span>])</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 0.37658867, -0.50925475,  0.64018118, ..., -0.54714549,
        -0.15865779,  2.18054509],
       [-1.1896863 ,  0.20437439,  0.48438287, ...,  0.48250091,
        -0.01657311, -0.11932991],
       [ 0.75707078, -0.70654362, -1.54245985, ..., -0.82144761,
         1.25550044, -0.39951387],
       ..., 
       [ 0.41898578, -1.61691248, -0.96906596, ...,  0.31491351,
         0.94449067, -1.63380706],
       [-1.87526524, -0.39853218,  0.41969284, ...,  0.87809855,
         0.70499468,  1.21494567],
       [ 1.91992784, -1.15394104,  2.01660442, ..., -1.89930511,
        -1.05463088, -0.38662028]], dtype=float32)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print FEATURES</span></span><br><span class="line"><span class="comment"># print mal_339[FEATURES].dtypes</span></span><br><span class="line">mal_339_features = mal_339[FEATURES]</span><br><span class="line"><span class="keyword">print</span> mal_339_features.iloc[:<span class="number">2</span>, :]</span><br><span class="line">mal_339_features = mal_339_features.fillna(<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">print</span> mal_339_features.iloc[:<span class="number">2</span>, :]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">mal_339_features = scaler.fit_transform(mal_339_features)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> mal_339_features[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># mal_339_np = mal_339_features.as_matrix()</span></span><br><span class="line"><span class="comment"># mal_339_np_notnan = np.nan_to_num(mal_339_np)</span></span><br><span class="line"><span class="comment"># print mal_339_np_notnan</span></span><br><span class="line"><span class="comment"># for i in mal_339_np_notnan:</span></span><br><span class="line"><span class="comment">#     for j in i:</span></span><br><span class="line"><span class="comment">#          if np.isnan(j):</span></span><br><span class="line"><span class="comment">#             print j</span></span><br></pre></td></tr></table></figure>
<pre><code>        Dur  Sport  TotPkts  TotBytes  SrcBytes     -&gt;    &lt;-    &lt;-&gt;    &lt;?&gt;  \
0  0.000000    0.0        1        60        60      1     0      0      0   
1  2.002936    NaN        3       126       126      0     0      0      0   

     who ...   SR_SA  S_  S_RA  TXD  UNK  URFIL  URH  URHPRO  URN  URP  
0      0 ...       0   0     0    0    0      0    0       0    0    0  
1      1 ...       0   0     0    0    0      0    0       0    0    0  

[2 rows x 58 columns]
        Dur  Sport  TotPkts  TotBytes  SrcBytes     -&gt;    &lt;-    &lt;-&gt;    &lt;?&gt;  \
0  0.000000    0.0        1        60        60      1     0      0      0   
1  2.002936   -1.0        3       126       126      0     0      0      0   

     who ...   SR_SA  S_  S_RA  TXD  UNK  URFIL  URH  URHPRO  URN  URP  
0      0 ...       0   0     0    0    0      0    0       0    0    0  
1      1 ...       0   0     0    0    0      0    0       0    0    0  

[2 rows x 58 columns]
[[  0.00000000e+00   1.52594876e-05   0.00000000e+00   4.06640181e-07
    1.02566206e-04   1.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00]
 [  5.56371187e-04   0.00000000e+00   5.04006854e-05   1.89765418e-06
    2.15389034e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># type(tf_df)</span></span><br><span class="line"><span class="comment"># mal_339_np[0:10]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_steps+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Prepare Data</span></span><br><span class="line">        <span class="comment"># Get the next batch of MNIST data (only images are needed, not labels)</span></span><br><span class="line">        <span class="comment"># batch_x, _ = mnist.train.next_batch(batch_size)</span></span><br><span class="line">        batch_x = mal_339_features[batch_size * (i<span class="number">-1</span>) : batch_size * i]</span><br><span class="line">        <span class="comment">#         batch_x = mal_339_np_notnan</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">        _, l = sess.run([optimizer, loss], feed_dict=&#123;X: batch_x&#125;)</span><br><span class="line">        <span class="comment"># Display logs per step</span></span><br><span class="line">        <span class="keyword">if</span> i % display_step == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">1</span>:</span><br><span class="line">            print(<span class="string">'Step %i: Minibatch Loss: %f'</span> % (i, l))</span><br><span class="line"><span class="comment">#             print sess.run(weights['encoder_h1'][:20])</span></span><br></pre></td></tr></table></figure>
<pre><code>Step 1: Minibatch Loss: 0.468984
Step 10: Minibatch Loss: 0.455147
Step 20: Minibatch Loss: 0.462695
Step 30: Minibatch Loss: 0.447134
Step 40: Minibatch Loss: 0.441346
Step 50: Minibatch Loss: 0.439445
Step 60: Minibatch Loss: 0.416833
Step 70: Minibatch Loss: 0.394649
Step 80: Minibatch Loss: 0.351088
Step 90: Minibatch Loss: 0.312108
Step 100: Minibatch Loss: 0.267290
Step 110: Minibatch Loss: 0.194041
Step 120: Minibatch Loss: 0.162867
Step 130: Minibatch Loss: 0.133697
Step 140: Minibatch Loss: 0.111308
Step 150: Minibatch Loss: 0.106732
Step 160: Minibatch Loss: 0.097534
Step 170: Minibatch Loss: 0.091810
Step 180: Minibatch Loss: 0.117968
Step 190: Minibatch Loss: 0.079072
Step 200: Minibatch Loss: 0.090758
Step 210: Minibatch Loss: 0.082447
Step 220: Minibatch Loss: 0.048609
Step 230: Minibatch Loss: 0.032585
Step 240: Minibatch Loss: 0.031063
Step 250: Minibatch Loss: 0.031056
Step 260: Minibatch Loss: 0.029959
Step 270: Minibatch Loss: 0.030090
Step 280: Minibatch Loss: 0.029585
Step 290: Minibatch Loss: 0.028629
Step 300: Minibatch Loss: 0.024438
Step 310: Minibatch Loss: 0.021379
Step 320: Minibatch Loss: 0.020640
Step 330: Minibatch Loss: 0.019872
Step 340: Minibatch Loss: 0.019863
Step 350: Minibatch Loss: 0.020726
Step 360: Minibatch Loss: 0.021458
Step 370: Minibatch Loss: 0.021579
Step 380: Minibatch Loss: 0.021721
Step 390: Minibatch Loss: 0.020004
Step 400: Minibatch Loss: 0.020803
Step 410: Minibatch Loss: 0.020184
Step 420: Minibatch Loss: 0.020479
Step 430: Minibatch Loss: 0.003755
Step 440: Minibatch Loss: 0.003437
Step 450: Minibatch Loss: 0.002824
Step 460: Minibatch Loss: 0.002821
Step 470: Minibatch Loss: 0.003415
Step 480: Minibatch Loss: 0.002324
Step 1: Minibatch Loss: 0.006036
Step 10: Minibatch Loss: 0.007076
Step 20: Minibatch Loss: 0.006751
Step 30: Minibatch Loss: 0.008192
Step 40: Minibatch Loss: 0.007363
Step 50: Minibatch Loss: 0.004416
Step 60: Minibatch Loss: 0.007745
Step 70: Minibatch Loss: 0.006650
Step 80: Minibatch Loss: 0.000743
Step 90: Minibatch Loss: 0.006038
Step 100: Minibatch Loss: 0.005925
Step 110: Minibatch Loss: 0.006763
Step 120: Minibatch Loss: 0.006746
Step 130: Minibatch Loss: 0.007101
Step 140: Minibatch Loss: 0.004960
Step 150: Minibatch Loss: 0.005270
Step 160: Minibatch Loss: 0.006832
Step 170: Minibatch Loss: 0.005422
Step 180: Minibatch Loss: 0.020940
Step 190: Minibatch Loss: 0.003963
Step 200: Minibatch Loss: 0.002104
Step 210: Minibatch Loss: 0.003122
Step 220: Minibatch Loss: 0.001235
Step 230: Minibatch Loss: 0.003156
Step 240: Minibatch Loss: 0.002204
Step 250: Minibatch Loss: 0.002980
Step 260: Minibatch Loss: 0.001901
Step 270: Minibatch Loss: 0.002672
Step 280: Minibatch Loss: 0.001595
Step 290: Minibatch Loss: 0.001164
Step 300: Minibatch Loss: 0.001884
Step 310: Minibatch Loss: 0.002769
Step 320: Minibatch Loss: 0.002188
Step 330: Minibatch Loss: 0.001608
Step 340: Minibatch Loss: 0.002238
Step 350: Minibatch Loss: 0.001578
Step 360: Minibatch Loss: 0.003141
Step 370: Minibatch Loss: 0.002898
Step 380: Minibatch Loss: 0.003277
Step 390: Minibatch Loss: 0.002074
Step 400: Minibatch Loss: 0.001717
Step 410: Minibatch Loss: 0.001788
Step 420: Minibatch Loss: 0.002390
Step 430: Minibatch Loss: 0.002212
Step 440: Minibatch Loss: 0.001443
Step 450: Minibatch Loss: 0.002267
Step 460: Minibatch Loss: 0.001611
Step 470: Minibatch Loss: 0.002712
Step 480: Minibatch Loss: 0.001602
Step 1: Minibatch Loss: 0.004985
Step 10: Minibatch Loss: 0.006228
Step 20: Minibatch Loss: 0.005845
Step 30: Minibatch Loss: 0.007771
Step 40: Minibatch Loss: 0.009470
Step 50: Minibatch Loss: 0.004150
Step 60: Minibatch Loss: 0.007376
Step 70: Minibatch Loss: 0.006216
Step 80: Minibatch Loss: 0.000589
Step 90: Minibatch Loss: 0.005752
Step 100: Minibatch Loss: 0.005568
Step 110: Minibatch Loss: 0.006289
Step 120: Minibatch Loss: 0.006357
Step 130: Minibatch Loss: 0.006889
Step 140: Minibatch Loss: 0.005090
Step 150: Minibatch Loss: 0.004828
Step 160: Minibatch Loss: 0.006685
Step 170: Minibatch Loss: 0.005279
Step 180: Minibatch Loss: 0.017944
Step 190: Minibatch Loss: 0.003714
Step 200: Minibatch Loss: 0.001734
Step 210: Minibatch Loss: 0.002454
Step 220: Minibatch Loss: 0.001408
Step 230: Minibatch Loss: 0.002204
Step 240: Minibatch Loss: 0.001627
Step 250: Minibatch Loss: 0.002121
Step 260: Minibatch Loss: 0.001691
Step 270: Minibatch Loss: 0.002266
Step 280: Minibatch Loss: 0.001269
Step 290: Minibatch Loss: 0.001065
Step 300: Minibatch Loss: 0.001836
Step 310: Minibatch Loss: 0.002391
Step 320: Minibatch Loss: 0.001977
Step 330: Minibatch Loss: 0.001707
Step 340: Minibatch Loss: 0.002218
Step 350: Minibatch Loss: 0.001268
Step 360: Minibatch Loss: 0.002706
Step 370: Minibatch Loss: 0.002965
Step 380: Minibatch Loss: 0.003375
Step 390: Minibatch Loss: 0.002017
Step 400: Minibatch Loss: 0.001735
Step 410: Minibatch Loss: 0.001580
Step 420: Minibatch Loss: 0.002336
Step 430: Minibatch Loss: 0.001910
Step 440: Minibatch Loss: 0.001460
Step 450: Minibatch Loss: 0.002207
Step 460: Minibatch Loss: 0.001573
Step 470: Minibatch Loss: 0.002799
Step 480: Minibatch Loss: 0.001548
Step 1: Minibatch Loss: 0.004929
Step 10: Minibatch Loss: 0.006213
Step 20: Minibatch Loss: 0.005819
Step 30: Minibatch Loss: 0.007757
Step 40: Minibatch Loss: 0.007016
Step 50: Minibatch Loss: 0.004122
Step 60: Minibatch Loss: 0.007372
Step 70: Minibatch Loss: 0.006208
Step 80: Minibatch Loss: 0.000740
Step 90: Minibatch Loss: 0.005741
Step 100: Minibatch Loss: 0.005553
Step 110: Minibatch Loss: 0.006272
Step 120: Minibatch Loss: 0.006352
Step 130: Minibatch Loss: 0.006884
Step 140: Minibatch Loss: 0.005009
Step 150: Minibatch Loss: 0.004554
Step 160: Minibatch Loss: 0.006688
Step 170: Minibatch Loss: 0.005268
Step 180: Minibatch Loss: 0.017967
Step 190: Minibatch Loss: 0.003721
Step 200: Minibatch Loss: 0.001642
Step 210: Minibatch Loss: 0.002569
Step 220: Minibatch Loss: 0.001270
Step 230: Minibatch Loss: 0.002216
Step 240: Minibatch Loss: 0.001557
Step 250: Minibatch Loss: 0.002055
Step 260: Minibatch Loss: 0.001748
Step 270: Minibatch Loss: 0.002337
Step 280: Minibatch Loss: 0.001296
Step 290: Minibatch Loss: 0.001052
Step 300: Minibatch Loss: 0.001837
Step 310: Minibatch Loss: 0.002364
Step 320: Minibatch Loss: 0.001969
Step 330: Minibatch Loss: 0.001718
Step 340: Minibatch Loss: 0.002196
Step 350: Minibatch Loss: 0.001240
Step 360: Minibatch Loss: 0.002712
Step 370: Minibatch Loss: 0.002751
Step 380: Minibatch Loss: 0.003312
Step 390: Minibatch Loss: 0.001994
Step 400: Minibatch Loss: 0.001733
Step 410: Minibatch Loss: 0.001591
Step 420: Minibatch Loss: 0.002297
Step 430: Minibatch Loss: 0.001898
Step 440: Minibatch Loss: 0.001438
Step 450: Minibatch Loss: 0.002181
Step 460: Minibatch Loss: 0.001568
Step 470: Minibatch Loss: 0.002731
Step 480: Minibatch Loss: 0.001518
Step 1: Minibatch Loss: 0.004764
Step 10: Minibatch Loss: 0.006205
Step 20: Minibatch Loss: 0.005802
Step 30: Minibatch Loss: 0.007750
Step 40: Minibatch Loss: 0.007062
Step 50: Minibatch Loss: 0.004117
Step 60: Minibatch Loss: 0.007375
Step 70: Minibatch Loss: 0.006210
Step 80: Minibatch Loss: 0.000705
Step 90: Minibatch Loss: 0.005731
Step 100: Minibatch Loss: 0.005557
Step 110: Minibatch Loss: 0.006150
Step 120: Minibatch Loss: 0.006216
Step 130: Minibatch Loss: 0.006748
Step 140: Minibatch Loss: 0.004826
Step 150: Minibatch Loss: 0.004354
Step 160: Minibatch Loss: 0.006618
Step 170: Minibatch Loss: 0.005201
Step 180: Minibatch Loss: 0.016752
Step 190: Minibatch Loss: 0.003638
Step 200: Minibatch Loss: 0.001837
Step 210: Minibatch Loss: 0.002322
Step 220: Minibatch Loss: 0.001246
Step 230: Minibatch Loss: 0.002165
Step 240: Minibatch Loss: 0.001548
Step 250: Minibatch Loss: 0.001884
Step 260: Minibatch Loss: 0.001659
Step 270: Minibatch Loss: 0.002275
Step 280: Minibatch Loss: 0.001264
Step 290: Minibatch Loss: 0.001006
Step 300: Minibatch Loss: 0.001617
Step 310: Minibatch Loss: 0.002318
Step 320: Minibatch Loss: 0.001944
Step 330: Minibatch Loss: 0.001581
Step 340: Minibatch Loss: 0.002094
Step 350: Minibatch Loss: 0.001295
Step 360: Minibatch Loss: 0.002780
Step 370: Minibatch Loss: 0.002705
Step 380: Minibatch Loss: 0.003277
Step 390: Minibatch Loss: 0.001961
Step 400: Minibatch Loss: 0.001861
Step 410: Minibatch Loss: 0.001507
Step 420: Minibatch Loss: 0.002161
Step 430: Minibatch Loss: 0.001933
Step 440: Minibatch Loss: 0.001377
Step 450: Minibatch Loss: 0.002096
Step 460: Minibatch Loss: 0.001849
Step 470: Minibatch Loss: 0.002678
Step 480: Minibatch Loss: 0.001540
Step 1: Minibatch Loss: 0.004416
Step 10: Minibatch Loss: 0.006004
Step 20: Minibatch Loss: 0.005666
Step 30: Minibatch Loss: 0.007551
Step 40: Minibatch Loss: 0.006895
Step 50: Minibatch Loss: 0.004075
Step 60: Minibatch Loss: 0.007240
Step 70: Minibatch Loss: 0.006069
Step 80: Minibatch Loss: 0.000659
Step 90: Minibatch Loss: 0.005537
Step 100: Minibatch Loss: 0.005411
Step 110: Minibatch Loss: 0.006136
Step 120: Minibatch Loss: 0.006210
Step 130: Minibatch Loss: 0.006742
Step 140: Minibatch Loss: 0.004826
Step 150: Minibatch Loss: 0.004231
Step 160: Minibatch Loss: 0.006617
Step 170: Minibatch Loss: 0.005198
Step 180: Minibatch Loss: 0.004377
Step 190: Minibatch Loss: 0.003690
Step 200: Minibatch Loss: 0.001735
Step 210: Minibatch Loss: 0.002312
Step 220: Minibatch Loss: 0.001228
Step 230: Minibatch Loss: 0.002182
Step 240: Minibatch Loss: 0.001513
Step 250: Minibatch Loss: 0.001876
Step 260: Minibatch Loss: 0.001641
Step 270: Minibatch Loss: 0.002241
Step 280: Minibatch Loss: 0.001263
Step 290: Minibatch Loss: 0.000996
Step 300: Minibatch Loss: 0.001557
Step 310: Minibatch Loss: 0.002268
Step 320: Minibatch Loss: 0.001921
Step 330: Minibatch Loss: 0.001588
Step 340: Minibatch Loss: 0.002094
Step 350: Minibatch Loss: 0.001269
Step 360: Minibatch Loss: 0.002771
Step 370: Minibatch Loss: 0.002687
Step 380: Minibatch Loss: 0.003268
Step 390: Minibatch Loss: 0.001940
Step 400: Minibatch Loss: 0.001843
Step 410: Minibatch Loss: 0.001547
Step 420: Minibatch Loss: 0.002178
Step 430: Minibatch Loss: 0.001916
Step 440: Minibatch Loss: 0.001369
Step 450: Minibatch Loss: 0.002094
Step 460: Minibatch Loss: 0.001611
Step 470: Minibatch Loss: 0.002643
Step 480: Minibatch Loss: 0.001525
Step 1: Minibatch Loss: 0.004335
Step 10: Minibatch Loss: 0.006004
Step 20: Minibatch Loss: 0.005664
Step 30: Minibatch Loss: 0.007549
Step 40: Minibatch Loss: 0.006881
Step 50: Minibatch Loss: 0.004051
Step 60: Minibatch Loss: 0.007229
Step 70: Minibatch Loss: 0.006071
Step 80: Minibatch Loss: 0.000680
Step 90: Minibatch Loss: 0.005537
Step 100: Minibatch Loss: 0.005403
Step 110: Minibatch Loss: 0.006133
Step 120: Minibatch Loss: 0.006204
Step 130: Minibatch Loss: 0.006744
Step 140: Minibatch Loss: 0.004755
Step 150: Minibatch Loss: 0.004234
Step 160: Minibatch Loss: 0.006632
Step 170: Minibatch Loss: 0.005195
Step 180: Minibatch Loss: 0.000788
Step 190: Minibatch Loss: 0.003595
Step 200: Minibatch Loss: 0.001754
Step 210: Minibatch Loss: 0.002321
Step 220: Minibatch Loss: 0.001242
Step 230: Minibatch Loss: 0.002144
Step 240: Minibatch Loss: 0.001515
Step 250: Minibatch Loss: 0.001893
Step 260: Minibatch Loss: 0.001638
Step 270: Minibatch Loss: 0.002232
Step 280: Minibatch Loss: 0.001262
Step 290: Minibatch Loss: 0.001012
Step 300: Minibatch Loss: 0.001571
Step 310: Minibatch Loss: 0.002310
Step 320: Minibatch Loss: 0.001922
Step 330: Minibatch Loss: 0.001573
Step 340: Minibatch Loss: 0.002094
Step 350: Minibatch Loss: 0.001284
Step 360: Minibatch Loss: 0.002756
Step 370: Minibatch Loss: 0.002682
Step 380: Minibatch Loss: 0.003272
Step 390: Minibatch Loss: 0.001920
Step 400: Minibatch Loss: 0.001830
Step 410: Minibatch Loss: 0.001564
Step 420: Minibatch Loss: 0.002186
Step 430: Minibatch Loss: 0.001904
Step 440: Minibatch Loss: 0.001367
Step 450: Minibatch Loss: 0.002093
Step 460: Minibatch Loss: 0.001566
Step 470: Minibatch Loss: 0.002640
Step 480: Minibatch Loss: 0.001520
Step 1: Minibatch Loss: 0.004264
Step 10: Minibatch Loss: 0.006009
Step 20: Minibatch Loss: 0.005664
Step 30: Minibatch Loss: 0.007551
Step 40: Minibatch Loss: 0.006880
Step 50: Minibatch Loss: 0.004052
Step 60: Minibatch Loss: 0.007225
Step 70: Minibatch Loss: 0.006069
Step 80: Minibatch Loss: 0.000683
Step 90: Minibatch Loss: 0.005535
Step 100: Minibatch Loss: 0.005400
Step 110: Minibatch Loss: 0.006133
Step 120: Minibatch Loss: 0.006207
Step 130: Minibatch Loss: 0.006740
Step 140: Minibatch Loss: 0.004784
Step 150: Minibatch Loss: 0.004210
Step 160: Minibatch Loss: 0.006633
Step 170: Minibatch Loss: 0.005194
Step 180: Minibatch Loss: 0.000747
Step 190: Minibatch Loss: 0.003600
Step 200: Minibatch Loss: 0.001717
Step 210: Minibatch Loss: 0.002309
Step 220: Minibatch Loss: 0.001210
Step 230: Minibatch Loss: 0.002195
Step 240: Minibatch Loss: 0.001488
Step 250: Minibatch Loss: 0.001921
Step 260: Minibatch Loss: 0.001637
Step 270: Minibatch Loss: 0.002229
Step 280: Minibatch Loss: 0.001304
Step 290: Minibatch Loss: 0.000987
Step 300: Minibatch Loss: 0.001558
Step 310: Minibatch Loss: 0.002256
Step 320: Minibatch Loss: 0.001914
Step 330: Minibatch Loss: 0.001582
Step 340: Minibatch Loss: 0.002095
Step 350: Minibatch Loss: 0.001256
Step 360: Minibatch Loss: 0.002751
Step 370: Minibatch Loss: 0.002694
Step 380: Minibatch Loss: 0.003270
Step 390: Minibatch Loss: 0.001915
Step 400: Minibatch Loss: 0.001824
Step 410: Minibatch Loss: 0.001582
Step 420: Minibatch Loss: 0.002184
Step 430: Minibatch Loss: 0.001898
Step 440: Minibatch Loss: 0.001367
Step 450: Minibatch Loss: 0.002093
Step 460: Minibatch Loss: 0.001559
Step 470: Minibatch Loss: 0.002639
Step 480: Minibatch Loss: 0.001503
Step 1: Minibatch Loss: 0.004237
Step 10: Minibatch Loss: 0.006009
Step 20: Minibatch Loss: 0.005663
Step 30: Minibatch Loss: 0.007552
Step 40: Minibatch Loss: 0.006890
Step 50: Minibatch Loss: 0.004049
Step 60: Minibatch Loss: 0.007220
Step 70: Minibatch Loss: 0.006067
Step 80: Minibatch Loss: 0.000605
Step 90: Minibatch Loss: 0.005532
Step 100: Minibatch Loss: 0.005404
Step 110: Minibatch Loss: 0.006134
Step 120: Minibatch Loss: 0.006215
Step 130: Minibatch Loss: 0.006739
Step 140: Minibatch Loss: 0.004765
Step 150: Minibatch Loss: 0.004200
Step 160: Minibatch Loss: 0.006648
Step 170: Minibatch Loss: 0.005194
Step 180: Minibatch Loss: 0.000488
Step 190: Minibatch Loss: 0.003588
Step 200: Minibatch Loss: 0.001738
Step 210: Minibatch Loss: 0.002307
Step 220: Minibatch Loss: 0.001209
Step 230: Minibatch Loss: 0.002184
Step 240: Minibatch Loss: 0.001488
Step 250: Minibatch Loss: 0.001939
Step 260: Minibatch Loss: 0.001624
Step 270: Minibatch Loss: 0.002227
Step 280: Minibatch Loss: 0.001302
Step 290: Minibatch Loss: 0.000990
Step 300: Minibatch Loss: 0.001560
Step 310: Minibatch Loss: 0.002254
Step 320: Minibatch Loss: 0.001913
Step 330: Minibatch Loss: 0.001583
Step 340: Minibatch Loss: 0.002100
Step 350: Minibatch Loss: 0.001262
Step 360: Minibatch Loss: 0.002744
Step 370: Minibatch Loss: 0.002679
Step 380: Minibatch Loss: 0.003253
Step 390: Minibatch Loss: 0.001943
Step 400: Minibatch Loss: 0.001869
Step 410: Minibatch Loss: 0.001571
Step 420: Minibatch Loss: 0.002184
Step 430: Minibatch Loss: 0.001898
Step 440: Minibatch Loss: 0.001364
Step 450: Minibatch Loss: 0.002093
Step 460: Minibatch Loss: 0.001550
Step 470: Minibatch Loss: 0.002639
Step 480: Minibatch Loss: 0.001517
Step 1: Minibatch Loss: 0.004196
Step 10: Minibatch Loss: 0.006006
Step 20: Minibatch Loss: 0.005662
Step 30: Minibatch Loss: 0.007551
Step 40: Minibatch Loss: 0.006894
Step 50: Minibatch Loss: 0.004050
Step 60: Minibatch Loss: 0.007217
Step 70: Minibatch Loss: 0.006068
Step 80: Minibatch Loss: 0.000712
Step 90: Minibatch Loss: 0.005531
Step 100: Minibatch Loss: 0.005398
Step 110: Minibatch Loss: 0.006136
Step 120: Minibatch Loss: 0.006207
Step 130: Minibatch Loss: 0.006739
Step 140: Minibatch Loss: 0.004769
Step 150: Minibatch Loss: 0.004185
Step 160: Minibatch Loss: 0.006643
Step 170: Minibatch Loss: 0.005192
Step 180: Minibatch Loss: 0.000454
Step 190: Minibatch Loss: 0.003586
Step 200: Minibatch Loss: 0.001721
Step 210: Minibatch Loss: 0.002307
Step 220: Minibatch Loss: 0.001207
Step 230: Minibatch Loss: 0.002172
Step 240: Minibatch Loss: 0.001498
Step 250: Minibatch Loss: 0.001940
Step 260: Minibatch Loss: 0.001624
Step 270: Minibatch Loss: 0.002216
Step 280: Minibatch Loss: 0.001306
Step 290: Minibatch Loss: 0.000990
Step 300: Minibatch Loss: 0.001559
Step 310: Minibatch Loss: 0.002255
Step 320: Minibatch Loss: 0.001912
Step 330: Minibatch Loss: 0.001580
Step 340: Minibatch Loss: 0.002094
Step 350: Minibatch Loss: 0.001256
Step 360: Minibatch Loss: 0.002744
Step 370: Minibatch Loss: 0.002772
Step 380: Minibatch Loss: 0.003259
Step 390: Minibatch Loss: 0.001913
Step 400: Minibatch Loss: 0.001871
Step 410: Minibatch Loss: 0.001579
Step 420: Minibatch Loss: 0.002181
Step 430: Minibatch Loss: 0.001894
Step 440: Minibatch Loss: 0.001362
Step 450: Minibatch Loss: 0.002093
Step 460: Minibatch Loss: 0.001544
Step 470: Minibatch Loss: 0.002645
Step 480: Minibatch Loss: 0.001512
Step 1: Minibatch Loss: 0.004177
Step 10: Minibatch Loss: 0.006005
Step 20: Minibatch Loss: 0.005662
Step 30: Minibatch Loss: 0.007551
Step 40: Minibatch Loss: 0.006928
Step 50: Minibatch Loss: 0.004051
Step 60: Minibatch Loss: 0.007215
Step 70: Minibatch Loss: 0.006068
Step 80: Minibatch Loss: 0.000742
Step 90: Minibatch Loss: 0.005529
Step 100: Minibatch Loss: 0.005397
Step 110: Minibatch Loss: 0.006134
Step 120: Minibatch Loss: 0.006209
Step 130: Minibatch Loss: 0.006738
Step 140: Minibatch Loss: 0.004744
Step 150: Minibatch Loss: 0.004178
Step 160: Minibatch Loss: 0.006628
Step 170: Minibatch Loss: 0.005192
Step 180: Minibatch Loss: 0.000343
Step 190: Minibatch Loss: 0.003587
Step 200: Minibatch Loss: 0.001697
Step 210: Minibatch Loss: 0.002304
Step 220: Minibatch Loss: 0.001253
Step 230: Minibatch Loss: 0.002183
Step 240: Minibatch Loss: 0.001487
Step 250: Minibatch Loss: 0.001943
Step 260: Minibatch Loss: 0.001617
Step 270: Minibatch Loss: 0.002213
Step 280: Minibatch Loss: 0.001308
Step 290: Minibatch Loss: 0.000988
Step 300: Minibatch Loss: 0.001564
Step 310: Minibatch Loss: 0.002255
Step 320: Minibatch Loss: 0.001912
Step 330: Minibatch Loss: 0.001585
Step 340: Minibatch Loss: 0.002100
Step 350: Minibatch Loss: 0.001250
Step 360: Minibatch Loss: 0.002743
Step 370: Minibatch Loss: 0.002761
Step 380: Minibatch Loss: 0.003257
Step 390: Minibatch Loss: 0.001915
Step 400: Minibatch Loss: 0.001861
Step 410: Minibatch Loss: 0.001594
Step 420: Minibatch Loss: 0.002180
Step 430: Minibatch Loss: 0.001894
Step 440: Minibatch Loss: 0.001361
Step 450: Minibatch Loss: 0.002093
Step 460: Minibatch Loss: 0.001544
Step 470: Minibatch Loss: 0.002640
Step 480: Minibatch Loss: 0.001523
Step 1: Minibatch Loss: 0.004191
Step 10: Minibatch Loss: 0.006000
Step 20: Minibatch Loss: 0.005662
Step 30: Minibatch Loss: 0.007546
Step 40: Minibatch Loss: 0.006962
Step 50: Minibatch Loss: 0.004052
Step 60: Minibatch Loss: 0.007215
Step 70: Minibatch Loss: 0.006067
Step 80: Minibatch Loss: 0.000763
Step 90: Minibatch Loss: 0.005528
Step 100: Minibatch Loss: 0.005396
Step 110: Minibatch Loss: 0.006136
Step 120: Minibatch Loss: 0.006201
Step 130: Minibatch Loss: 0.006739
Step 140: Minibatch Loss: 0.004746
Step 150: Minibatch Loss: 0.004166
Step 160: Minibatch Loss: 0.006636
Step 170: Minibatch Loss: 0.005191
Step 180: Minibatch Loss: 0.000397
Step 190: Minibatch Loss: 0.003583
Step 200: Minibatch Loss: 0.001771
Step 210: Minibatch Loss: 0.002316
Step 220: Minibatch Loss: 0.001221
Step 230: Minibatch Loss: 0.002174
Step 240: Minibatch Loss: 0.001489
Step 250: Minibatch Loss: 0.001945
Step 260: Minibatch Loss: 0.001619
Step 270: Minibatch Loss: 0.002206
Step 280: Minibatch Loss: 0.001314
Step 290: Minibatch Loss: 0.000989
Step 300: Minibatch Loss: 0.001561
Step 310: Minibatch Loss: 0.002255
Step 320: Minibatch Loss: 0.001911
Step 330: Minibatch Loss: 0.001584
Step 340: Minibatch Loss: 0.002100
Step 350: Minibatch Loss: 0.001247
Step 360: Minibatch Loss: 0.002742
Step 370: Minibatch Loss: 0.002791
Step 380: Minibatch Loss: 0.003256
Step 390: Minibatch Loss: 0.001908
Step 400: Minibatch Loss: 0.001862
Step 410: Minibatch Loss: 0.001587
Step 420: Minibatch Loss: 0.002177
Step 430: Minibatch Loss: 0.001891
Step 440: Minibatch Loss: 0.001359
Step 450: Minibatch Loss: 0.002093
Step 460: Minibatch Loss: 0.001539
Step 470: Minibatch Loss: 0.002639
Step 480: Minibatch Loss: 0.001489
Step 1: Minibatch Loss: 0.004184
Step 10: Minibatch Loss: 0.006005
Step 20: Minibatch Loss: 0.005662
Step 30: Minibatch Loss: 0.007546
Step 40: Minibatch Loss: 0.006937
Step 50: Minibatch Loss: 0.004053
Step 60: Minibatch Loss: 0.007215
Step 70: Minibatch Loss: 0.006067
Step 80: Minibatch Loss: 0.000564
Step 90: Minibatch Loss: 0.005527
Step 100: Minibatch Loss: 0.005395
Step 110: Minibatch Loss: 0.006131
Step 120: Minibatch Loss: 0.006205
Step 130: Minibatch Loss: 0.006738
Step 140: Minibatch Loss: 0.004739
Step 150: Minibatch Loss: 0.004105
Step 160: Minibatch Loss: 0.006630
Step 170: Minibatch Loss: 0.005203
Step 180: Minibatch Loss: 0.000285
Step 190: Minibatch Loss: 0.003582
Step 200: Minibatch Loss: 0.001724
Step 210: Minibatch Loss: 0.002307
Step 220: Minibatch Loss: 0.001213
Step 230: Minibatch Loss: 0.002159
Step 240: Minibatch Loss: 0.001471
Step 250: Minibatch Loss: 0.001938
Step 260: Minibatch Loss: 0.001620
Step 270: Minibatch Loss: 0.002202
Step 280: Minibatch Loss: 0.001309
Step 290: Minibatch Loss: 0.000988
Step 300: Minibatch Loss: 0.001566
Step 310: Minibatch Loss: 0.002255
Step 320: Minibatch Loss: 0.001911
Step 330: Minibatch Loss: 0.001583
Step 340: Minibatch Loss: 0.002100
Step 350: Minibatch Loss: 0.001244
Step 360: Minibatch Loss: 0.002743
Step 370: Minibatch Loss: 0.002683
Step 380: Minibatch Loss: 0.003253
Step 390: Minibatch Loss: 0.001906
Step 400: Minibatch Loss: 0.001807
Step 410: Minibatch Loss: 0.001592
Step 420: Minibatch Loss: 0.002177
Step 430: Minibatch Loss: 0.001890
Step 440: Minibatch Loss: 0.001358
Step 450: Minibatch Loss: 0.002093
Step 460: Minibatch Loss: 0.001542
Step 470: Minibatch Loss: 0.002643
Step 480: Minibatch Loss: 0.001514
Step 1: Minibatch Loss: 0.004172
Step 10: Minibatch Loss: 0.006001
Step 20: Minibatch Loss: 0.005662
Step 30: Minibatch Loss: 0.007549
Step 40: Minibatch Loss: 0.006942
Step 50: Minibatch Loss: 0.004051
Step 60: Minibatch Loss: 0.007216
Step 70: Minibatch Loss: 0.006066
Step 80: Minibatch Loss: 0.000548
Step 90: Minibatch Loss: 0.005529
Step 100: Minibatch Loss: 0.005401
Step 110: Minibatch Loss: 0.006132
Step 120: Minibatch Loss: 0.006205
Step 130: Minibatch Loss: 0.006738
Step 140: Minibatch Loss: 0.004749
Step 150: Minibatch Loss: 0.004162
Step 160: Minibatch Loss: 0.006618
Step 170: Minibatch Loss: 0.005191
Step 180: Minibatch Loss: 0.000449
Step 190: Minibatch Loss: 0.003581
Step 200: Minibatch Loss: 0.001669
Step 210: Minibatch Loss: 0.002307
Step 220: Minibatch Loss: 0.001206
Step 230: Minibatch Loss: 0.002191
Step 240: Minibatch Loss: 0.001503
Step 250: Minibatch Loss: 0.001974
Step 260: Minibatch Loss: 0.001634
Step 270: Minibatch Loss: 0.002196
Step 280: Minibatch Loss: 0.001326
Step 290: Minibatch Loss: 0.000988
Step 300: Minibatch Loss: 0.001568
Step 310: Minibatch Loss: 0.002252
Step 320: Minibatch Loss: 0.001913
Step 330: Minibatch Loss: 0.001579
Step 340: Minibatch Loss: 0.002092
Step 350: Minibatch Loss: 0.001240
Step 360: Minibatch Loss: 0.002741
Step 370: Minibatch Loss: 0.002680
Step 380: Minibatch Loss: 0.003254
Step 390: Minibatch Loss: 0.001901
Step 400: Minibatch Loss: 0.001804
Step 410: Minibatch Loss: 0.001595
Step 420: Minibatch Loss: 0.002177
Step 430: Minibatch Loss: 0.001887
Step 440: Minibatch Loss: 0.001356
Step 450: Minibatch Loss: 0.002093
Step 460: Minibatch Loss: 0.001540
Step 470: Minibatch Loss: 0.002639
Step 480: Minibatch Loss: 0.001515
Step 1: Minibatch Loss: 0.004170
Step 10: Minibatch Loss: 0.006000
Step 20: Minibatch Loss: 0.005661
Step 30: Minibatch Loss: 0.007548
Step 40: Minibatch Loss: 0.006944
Step 50: Minibatch Loss: 0.004054
Step 60: Minibatch Loss: 0.007214
Step 70: Minibatch Loss: 0.006067
Step 80: Minibatch Loss: 0.000605
Step 90: Minibatch Loss: 0.005528
Step 100: Minibatch Loss: 0.005395
Step 110: Minibatch Loss: 0.006131
Step 120: Minibatch Loss: 0.006206
Step 130: Minibatch Loss: 0.006738
Step 140: Minibatch Loss: 0.004734
Step 150: Minibatch Loss: 0.004107
Step 160: Minibatch Loss: 0.006634
Step 170: Minibatch Loss: 0.005198
Step 180: Minibatch Loss: 0.000345
Step 190: Minibatch Loss: 0.003581
Step 200: Minibatch Loss: 0.001684
Step 210: Minibatch Loss: 0.002394
Step 220: Minibatch Loss: 0.001193
Step 230: Minibatch Loss: 0.002156
Step 240: Minibatch Loss: 0.001486
Step 250: Minibatch Loss: 0.001953
Step 260: Minibatch Loss: 0.001621
Step 270: Minibatch Loss: 0.002166
Step 280: Minibatch Loss: 0.001324
Step 290: Minibatch Loss: 0.001002
Step 300: Minibatch Loss: 0.001584
Step 310: Minibatch Loss: 0.002262
Step 320: Minibatch Loss: 0.001900
Step 330: Minibatch Loss: 0.001590
Step 340: Minibatch Loss: 0.002119
Step 350: Minibatch Loss: 0.001230
Step 360: Minibatch Loss: 0.002756
Step 370: Minibatch Loss: 0.002679
Step 380: Minibatch Loss: 0.003223
Step 390: Minibatch Loss: 0.001865
Step 400: Minibatch Loss: 0.001761
Step 410: Minibatch Loss: 0.001547
Step 420: Minibatch Loss: 0.002167
Step 430: Minibatch Loss: 0.001877
Step 440: Minibatch Loss: 0.001329
Step 450: Minibatch Loss: 0.002101
Step 460: Minibatch Loss: 0.001499
Step 470: Minibatch Loss: 0.002641
Step 480: Minibatch Loss: 0.001487
Step 1: Minibatch Loss: 0.004184
Step 10: Minibatch Loss: 0.006003
Step 20: Minibatch Loss: 0.005661
Step 30: Minibatch Loss: 0.007546
Step 40: Minibatch Loss: 0.006923
Step 50: Minibatch Loss: 0.004051
Step 60: Minibatch Loss: 0.007214
Step 70: Minibatch Loss: 0.006065
Step 80: Minibatch Loss: 0.000547
Step 90: Minibatch Loss: 0.005528
Step 100: Minibatch Loss: 0.005399
Step 110: Minibatch Loss: 0.006133
Step 120: Minibatch Loss: 0.006208
Step 130: Minibatch Loss: 0.006738
Step 140: Minibatch Loss: 0.004738
Step 150: Minibatch Loss: 0.004162
Step 160: Minibatch Loss: 0.006626
Step 170: Minibatch Loss: 0.005191
Step 180: Minibatch Loss: 0.000168
Step 190: Minibatch Loss: 0.003577
Step 200: Minibatch Loss: 0.001759
Step 210: Minibatch Loss: 0.002303
Step 220: Minibatch Loss: 0.001208
Step 230: Minibatch Loss: 0.002147
Step 240: Minibatch Loss: 0.001472
Step 250: Minibatch Loss: 0.001907
Step 260: Minibatch Loss: 0.001619
Step 270: Minibatch Loss: 0.002184
Step 280: Minibatch Loss: 0.001276
Step 290: Minibatch Loss: 0.000986
Step 300: Minibatch Loss: 0.001568
Step 310: Minibatch Loss: 0.002253
Step 320: Minibatch Loss: 0.001908
Step 330: Minibatch Loss: 0.001573
Step 340: Minibatch Loss: 0.002101
Step 350: Minibatch Loss: 0.001243
Step 360: Minibatch Loss: 0.002739
Step 370: Minibatch Loss: 0.002698
Step 380: Minibatch Loss: 0.003244
Step 390: Minibatch Loss: 0.001899
Step 400: Minibatch Loss: 0.001846
Step 410: Minibatch Loss: 0.001597
Step 420: Minibatch Loss: 0.002171
Step 430: Minibatch Loss: 0.001884
Step 440: Minibatch Loss: 0.001350
Step 450: Minibatch Loss: 0.002092
Step 460: Minibatch Loss: 0.001534
Step 470: Minibatch Loss: 0.002636
Step 480: Minibatch Loss: 0.001487
Step 1: Minibatch Loss: 0.004166
Step 10: Minibatch Loss: 0.006002
Step 20: Minibatch Loss: 0.005661
Step 30: Minibatch Loss: 0.007548
Step 40: Minibatch Loss: 0.006940
Step 50: Minibatch Loss: 0.004050
Step 60: Minibatch Loss: 0.007213
Step 70: Minibatch Loss: 0.006065
Step 80: Minibatch Loss: 0.000547
Step 90: Minibatch Loss: 0.005528
Step 100: Minibatch Loss: 0.005398
Step 110: Minibatch Loss: 0.006134
Step 120: Minibatch Loss: 0.006199
Step 130: Minibatch Loss: 0.006741
Step 140: Minibatch Loss: 0.004739
Step 150: Minibatch Loss: 0.004145
Step 160: Minibatch Loss: 0.006625
Step 170: Minibatch Loss: 0.005190
Step 180: Minibatch Loss: 0.000200
Step 190: Minibatch Loss: 0.003604
Step 200: Minibatch Loss: 0.001708
Step 210: Minibatch Loss: 0.002301
Step 220: Minibatch Loss: 0.001203
Step 230: Minibatch Loss: 0.002157
Step 240: Minibatch Loss: 0.001468
Step 250: Minibatch Loss: 0.001884
Step 260: Minibatch Loss: 0.001608
Step 270: Minibatch Loss: 0.002166
Step 280: Minibatch Loss: 0.001310
Step 290: Minibatch Loss: 0.000984
Step 300: Minibatch Loss: 0.001571
Step 310: Minibatch Loss: 0.002250
Step 320: Minibatch Loss: 0.001907
Step 330: Minibatch Loss: 0.001576
Step 340: Minibatch Loss: 0.002099
Step 350: Minibatch Loss: 0.001237
Step 360: Minibatch Loss: 0.002737
Step 370: Minibatch Loss: 0.002822
Step 380: Minibatch Loss: 0.003247
Step 390: Minibatch Loss: 0.001891
Step 400: Minibatch Loss: 0.001836
Step 410: Minibatch Loss: 0.001593
Step 420: Minibatch Loss: 0.002171
Step 430: Minibatch Loss: 0.001882
Step 440: Minibatch Loss: 0.001348
Step 450: Minibatch Loss: 0.002092
Step 460: Minibatch Loss: 0.001533
Step 470: Minibatch Loss: 0.002635
Step 480: Minibatch Loss: 0.001488
Step 1: Minibatch Loss: 0.004169
Step 10: Minibatch Loss: 0.006002
Step 20: Minibatch Loss: 0.005661
Step 30: Minibatch Loss: 0.007547
Step 40: Minibatch Loss: 0.006926
Step 50: Minibatch Loss: 0.004049
Step 60: Minibatch Loss: 0.007212
Step 70: Minibatch Loss: 0.006065
Step 80: Minibatch Loss: 0.000547
Step 90: Minibatch Loss: 0.005528
Step 100: Minibatch Loss: 0.005398
Step 110: Minibatch Loss: 0.006136
Step 120: Minibatch Loss: 0.006212
Step 130: Minibatch Loss: 0.006738
Step 140: Minibatch Loss: 0.004746
Step 150: Minibatch Loss: 0.004126
Step 160: Minibatch Loss: 0.006610
Step 170: Minibatch Loss: 0.005209
Step 180: Minibatch Loss: 0.000411
Step 190: Minibatch Loss: 0.003582
Step 200: Minibatch Loss: 0.001629
Step 210: Minibatch Loss: 0.002393
Step 220: Minibatch Loss: 0.001198
Step 230: Minibatch Loss: 0.002158
Step 240: Minibatch Loss: 0.001480
Step 250: Minibatch Loss: 0.001935
Step 260: Minibatch Loss: 0.001628
Step 270: Minibatch Loss: 0.002155
Step 280: Minibatch Loss: 0.001321
Step 290: Minibatch Loss: 0.001001
Step 300: Minibatch Loss: 0.001579
Step 310: Minibatch Loss: 0.002259
Step 320: Minibatch Loss: 0.001900
Step 330: Minibatch Loss: 0.001582
Step 340: Minibatch Loss: 0.002102
Step 350: Minibatch Loss: 0.001228
Step 360: Minibatch Loss: 0.002745
Step 370: Minibatch Loss: 0.002674
Step 380: Minibatch Loss: 0.003223
Step 390: Minibatch Loss: 0.001864
Step 400: Minibatch Loss: 0.001802
Step 410: Minibatch Loss: 0.001549
Step 420: Minibatch Loss: 0.002163
Step 430: Minibatch Loss: 0.001875
Step 440: Minibatch Loss: 0.001327
Step 450: Minibatch Loss: 0.002101
Step 460: Minibatch Loss: 0.001499
Step 470: Minibatch Loss: 0.002638
Step 480: Minibatch Loss: 0.001486
Step 1: Minibatch Loss: 0.004183
Step 10: Minibatch Loss: 0.006003
Step 20: Minibatch Loss: 0.005661
Step 30: Minibatch Loss: 0.007547
Step 40: Minibatch Loss: 0.006925
Step 50: Minibatch Loss: 0.004049
Step 60: Minibatch Loss: 0.007212
Step 70: Minibatch Loss: 0.006065
Step 80: Minibatch Loss: 0.000547
Step 90: Minibatch Loss: 0.005527
Step 100: Minibatch Loss: 0.005397
Step 110: Minibatch Loss: 0.006134
Step 120: Minibatch Loss: 0.006207
Step 130: Minibatch Loss: 0.006737
Step 140: Minibatch Loss: 0.004740
Step 150: Minibatch Loss: 0.004138
Step 160: Minibatch Loss: 0.006618
Step 170: Minibatch Loss: 0.005190
Step 180: Minibatch Loss: 0.000158
Step 190: Minibatch Loss: 0.003597
Step 200: Minibatch Loss: 0.001574
Step 210: Minibatch Loss: 0.002303
Step 220: Minibatch Loss: 0.001211
Step 230: Minibatch Loss: 0.002146
Step 240: Minibatch Loss: 0.001462
Step 250: Minibatch Loss: 0.001875
Step 260: Minibatch Loss: 0.001604
Step 270: Minibatch Loss: 0.002155
Step 280: Minibatch Loss: 0.001306
Step 290: Minibatch Loss: 0.000984
Step 300: Minibatch Loss: 0.001567
Step 310: Minibatch Loss: 0.002247
Step 320: Minibatch Loss: 0.001906
Step 330: Minibatch Loss: 0.001572
Step 340: Minibatch Loss: 0.002099
Step 350: Minibatch Loss: 0.001236
Step 360: Minibatch Loss: 0.002737
Step 370: Minibatch Loss: 0.002743
Step 380: Minibatch Loss: 0.003243
Step 390: Minibatch Loss: 0.001890
Step 400: Minibatch Loss: 0.001790
Step 410: Minibatch Loss: 0.001607
Step 420: Minibatch Loss: 0.002168
Step 430: Minibatch Loss: 0.001881
Step 440: Minibatch Loss: 0.001347
Step 450: Minibatch Loss: 0.002092
Step 460: Minibatch Loss: 0.001528
Step 470: Minibatch Loss: 0.002635
Step 480: Minibatch Loss: 0.001487
Step 1: Minibatch Loss: 0.004164
Step 10: Minibatch Loss: 0.006003
Step 20: Minibatch Loss: 0.005660
Step 30: Minibatch Loss: 0.007547
Step 40: Minibatch Loss: 0.006937
Step 50: Minibatch Loss: 0.004049
Step 60: Minibatch Loss: 0.007212
Step 70: Minibatch Loss: 0.006065
Step 80: Minibatch Loss: 0.000546
Step 90: Minibatch Loss: 0.005527
Step 100: Minibatch Loss: 0.005397
Step 110: Minibatch Loss: 0.006132
Step 120: Minibatch Loss: 0.006217
Step 130: Minibatch Loss: 0.006738
Step 140: Minibatch Loss: 0.004743
Step 150: Minibatch Loss: 0.004119
Step 160: Minibatch Loss: 0.006607
Step 170: Minibatch Loss: 0.005223
Step 180: Minibatch Loss: 0.000215
Step 190: Minibatch Loss: 0.003577
Step 200: Minibatch Loss: 0.001584
Step 210: Minibatch Loss: 0.002299
Step 220: Minibatch Loss: 0.001264
Step 230: Minibatch Loss: 0.002155
Step 240: Minibatch Loss: 0.001462
Step 250: Minibatch Loss: 0.001880
Step 260: Minibatch Loss: 0.001604
Step 270: Minibatch Loss: 0.002161
Step 280: Minibatch Loss: 0.001308
Step 290: Minibatch Loss: 0.000983
Step 300: Minibatch Loss: 0.001567
Step 310: Minibatch Loss: 0.002246
Step 320: Minibatch Loss: 0.001906
Step 330: Minibatch Loss: 0.001573
Step 340: Minibatch Loss: 0.002097
Step 350: Minibatch Loss: 0.001235
Step 360: Minibatch Loss: 0.002736
Step 370: Minibatch Loss: 0.002762
Step 380: Minibatch Loss: 0.003242
Step 390: Minibatch Loss: 0.001890
Step 400: Minibatch Loss: 0.001815
Step 410: Minibatch Loss: 0.001594
Step 420: Minibatch Loss: 0.002166
Step 430: Minibatch Loss: 0.001880
Step 440: Minibatch Loss: 0.001345
Step 450: Minibatch Loss: 0.002093
Step 460: Minibatch Loss: 0.001525
Step 470: Minibatch Loss: 0.002635
Step 480: Minibatch Loss: 0.001485
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mal_339_features.shape</span><br><span class="line"><span class="comment"># mal_339_features[1]</span></span><br></pre></td></tr></table></figure>
<pre><code>(123565, 58)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># loss_array = []</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Testing</span></span><br><span class="line"><span class="comment"># Encode and decode images from test set and visualize their reconstruction.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for index_array in xrange(0, mal_339_features.shape[0]):</span></span><br><span class="line"><span class="keyword">for</span> index_array <span class="keyword">in</span> xrange(<span class="number">0</span>,<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># Encode and decode </span></span><br><span class="line">    batch_x = mal_339_features</span><br><span class="line">    g = sess.run(decoder_op, feed_dict=&#123;X: batch_x&#125;)</span><br><span class="line">    loss_array = np.sqrt(np.sum((g-mal_339_features)**<span class="number">2</span>,axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sess.run(loss_array)</span></span><br><span class="line"><span class="comment"># loss_final = sess.run(loss_array)</span></span><br><span class="line">type(loss_array)</span><br><span class="line"><span class="comment"># print np.arange(len(loss_final))</span></span><br></pre></td></tr></table></figure>
<pre><code>numpy.ndarray
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># sns.barplot(x = range(0, len(loss_array), y = loss_array))</span></span><br><span class="line">plt.scatter(np.arange(len(loss_array)), loss_array)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_23_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(loss_array, bins=<span class="number">100</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_24_0.png" alt="png"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/learning/" rel="tag"># learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/15/first/" rel="next" title="TensorFlow Notes 1">
                <i class="fa fa-chevron-left"></i> TensorFlow Notes 1
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Jun</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jun</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("zyEXuUmobI6z6TNzwqqMH1hH-gzGzoHsz", "9O0pPQflIjbIF11tBGkMkHV9");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  

</body>
</html>
